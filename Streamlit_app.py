import pandas as pd
import streamlit as st
import pickle
from sqlalchemy import create_engine, MetaData, text
from sqlalchemy.dialects.postgresql import insert
import numpy as np
import streamlit as st
import os

from sqlalchemy import create_engine

# Supabase DB ì ‘ì† ë¬¸ìì—´ (ë¹„ë°€ë²ˆí˜¸ëŠ” ì‹¤ì œ ê°’ìœ¼ë¡œ êµì²´!)
DATABASE_URL = "postgresql+psycopg2://postgres:<ë¹„ë°€ë²ˆí˜¸>@db.fjaxvaegmtbsyogavuzy.supabase.co:5432/postgres?sslmode=require"

try:
    engine = create_engine(DATABASE_URL)
    with engine.connect() as conn:
        result = conn.execute("SELECT version();")
        for row in result:
            print("âœ… ì—°ê²° ì„±ê³µ:", row[0])
except Exception as e:
    print("âŒ ì—°ê²° ì‹¤íŒ¨:", e)


    
st.title("RetainFlow Automation: Customer Churn Prediction")


# íƒ­ ìƒì„±
tab1, tab2, tab3, tab4, tab5= st.tabs(["Problem", "EDA", "Modeling/Evaluation", "Application", "Outcome"])



with tab1:

    # Objective
    st.header("Objective")
    st.markdown("""
    - Identify churn-prone customers using historical telco data  
    - Automate retention workflows by connecting predictions to **make.com**  
    - Enable timely customer engagement (e.g., sending offers, alerts to CRM, or triggering support tickets)  
    """)


    st.header("Business Problem")
    st.markdown("""
    - Telecom customers can easily switch, so **churn directly reduces revenue**.  
    - Even a small improvement matters: a **5% reduction in churn can boost profits by 25â€“95%** (Bain & Co.).  
    - **Customer Acquisition Cost (CAC)** is about **5x higher** than **Customer Retention Cost (CRC)** (HBR).  
    - Despite this, many churn analyses stop at *prediction*, without seamless integration into **real business workflows**.  
    - Marketing and support teams often lack **real-time triggers** to act on churn insights.  
    - Without automation, valuable time is lost between churn detection and customer outreach.  
    - **Key challenge**: How well churn is predicted **and** how effectively marketing actions are automated determines revenue growth.  
    - This project bridges the gap by combining **machine learning models** with **make.com automation**, enabling organizations to not only *predict churn* but also to **automatically take retention actions**.  
    """)



    st.header("Telco Opportunity Map")
    st.image("pic.png", caption="Opportunity Map: Balancing Effort vs. Benefit", width=700)

    st.markdown("""
This opportunity map helps us **prioritize projects**:  
- **Top-left (Quick Wins)**: High benefit, low effort â†’ e.g., Churn AI + Discounts, Loyalty programs.  
- **Bottom-right (Long-term Bets)**: High effort, uncertain benefit â†’ e.g., IoT, 5G marketing.  
- Our focus starts with **Churn AI**, where benefit is high and execution is feasible.  
    """)



    st.header("Why It Matters")
    st.markdown("""
- **Cost efficiency**: Retaining is cheaper & more profitable.  
- **Customer lifetime value**: Identify & prioritize high-value churn-risk customers.  
- **Personalized marketing**: Segmentation + churn probability â†’ tailored offers.  
- **Revenue impact**: Proactive churn management drives growth.  
- **Strategic decisions**: Data-driven CRM, bundles, loyalty programs.  
    """)




    # ì°¸ê³ ë¬¸í—Œ (ì‘ê²Œ í‘œì‹œ)
    st.markdown(
        """
        <sub>**References**:  
        Bain & Company, *Customer Retention Economics*  
        Harvard Business Review (2014), *The Value of Keeping the Right Customers*</sub>
        """,
        unsafe_allow_html=True
    )














with tab2:


    
    st.header("Exploratory Data Analysis Results")




    st.markdown("""
                

    ### Data source / collection / challenges

    **Data source**  
    - Telco Customer Churn dataset (Kaggle / IBM Sample Data)  
    - Includes customer contracts, payment methods, service usage, billing history, and churn labels  

    **Data collection**  
    - Mainly gathered from **Customer Relationship Management (CRM)** and **Billing systems**  
    - Key feature categories:  
    - **Customer info**: tenure, SeniorCitizen  
    - **Contract info**: Contract, PaymentMethod, InternetService  
    - **Service usage**: TechSupport, OnlineSecurity, StreamingTV  
    - **Billing data**: MonthlyCharges, TotalCharges  
    - Churn is defined as whether a customer discontinued the service within a given period  

    **Challenges**  
    - **Data quality issues**: missing or invalid `TotalCharges` values â†’ preprocessing required  
    - **Class imbalance**: majority of customers are `Churn=No`, while `Churn=Yes` is a minority â†’ risk of biased models  
    - **Categorical features**: contract, payment method, and service usage require encoding for ML models  
    - **Limitations in realism**:  
    - Lacks behavioral data such as complaints, service quality issues, or customer interactions  
    - No information on customer re-subscription after churn or the impact of specific marketing campaigns  
    - Therefore, assessing marketing effectiveness and retention strategies is challenging with this dataset

    ---

                
    ### Key Insights from EDA

    **Customer Tenure**  
    - Customers with shorter tenure show a higher churn rate  
    - Long-term customers tend to have higher TotalCharges and lower churn probability  

    **Billing Metrics**  
    - Higher MonthlyCharges are associated with higher churn  
    - In contrast, TotalCharges show a negative correlation with churn, reflecting stronger customer loyalty  

    **Contract & Service Features**  
    - Month-to-month contracts have the highest churn rate  
    - Customers paying via **Electronic check** are more likely to churn  
    - Customers without **TechSupport / OnlineSecurity** services show significantly higher churn  

    """)











    eda_path = r"C:\Users\honor\spicedAcademy\Capstone_Final_Project\Retain_Flow_Automation-\notebook\notebook\eda_insight"

    if os.path.exists(eda_path):
        img_files = [f for f in os.listdir(eda_path) if f.endswith((".png", ".jpg", ".jpeg"))]

        if img_files:
            for img in img_files:
                st.image(
                    os.path.join(eda_path, img),
                    caption=img,
                    use_container_width=True  # âœ… ë³€ê²½ë¨
                )
        else:
            st.warning("âš ï¸ EDA ì´ë¯¸ì§€ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
    else:
        st.error("âŒ EDA ê²½ë¡œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê²½ë¡œë¥¼ ë‹¤ì‹œ í™•ì¸í•´ì£¼ì„¸ìš”.")



with tab3:




    st.header("Churn Methodology and Technology Stack Used")

    methodology_data = [
        ["Problem Definition", 
         "Predict customer churn in advance and identify high-value customers â†’ design targeted retention strategies"],
        ["Data Preparation", 
         "- Source: customer_Info copy.csv\n- Convert TotalCharges to numeric & handle missing values\n- Transform Churn: Yes/No â†’ 1/0"],
        ["Preprocessing Strategy", 
         "- Numerical: tenure, MonthlyCharges, TotalCharges\n- Categorical: Contract type, Payment method, etc. â†’ OneHotEncoding\n- Numeric cleaning with FunctionTransformer"],
        ["Imbalance Handling", 
         "SMOTE: oversampling of minority class (churned customers)"],
        ["Process", 
         "Model training â†’ Hyperparameter tuning â†’ Probability calibration â†’ Customer segmentation"],
        ["Technology Stack", 
         "- Python (pandas, numpy, scikit-learn, imbalanced-learn, seaborn, matplotlib)\n"
         "- Modeling: Pipeline, OneHotEncoder, ColumnTransformer, FunctionTransformer\n"
         "- Model: RandomForestClassifier + CalibratedClassifierCV\n"
         "- Optimization: GridSearchCV + StratifiedKFold (scoring=recall)\n"
         "- Imbalance: SMOTE\n"
         "- Clustering: KMeans + StandardScaler\n"
         "- Deployment: cloudpickle (model + scaler + clusterer bundle)"]
    ]
    methodology_df = pd.DataFrame(methodology_data, columns=["Section", "Details"])
    st.table(methodology_df)   # âœ… ìë™ ì¤„ë°”ê¿ˆ í‘œ

    st.header("Modelling")

    modelling_data = [
        ["Base Model", "RandomForestClassifier (class_weight='balanced')"],
        ["Hyperparameter Tuning", 
         "GridSearchCV (n_estimators, max_depth, min_samples_split, max_features)\n"
         "5-Fold Stratified CV, optimized for Recall"],
        ["Why Recall", 
         "Missing churners is more costly for the business than false positives"],
        ["Performance Improvements", 
         "- CalibratedClassifierCV: probability calibration (sigmoid)\n"
         "- Threshold adjustment: 0.5 vs 0.3 â†’ improves Recall\n"
         "- ROC Curve: AUC â‰ˆ 0.85"],
        ["Feature Importance", 
         "- Top features: MonthlyCharges, tenure, Contract, InternetService, OnlineSecurity â€¦\n"
         "- Insights:\n"
         "  â€¢ High MonthlyCharges + short tenure â†’ higher churn risk\n"
         "  â€¢ Missing add-ons (TechSupport, OnlineSecurity) â†’ higher churn risk"],
        ["Segmentation (KMeans)", 
         "- Input: Predicted churn probability + MonthlyCharges\n"
         "- Result: 4 clusters\n"
         "  â€¢ Cluster 2: High Risk & High Value (priority customers)\n"
         "  â€¢ Cluster 0/1: Low Risk groups\n"
         "- Use case: targeted marketing strategies per segment"]
    ]
    modelling_df = pd.DataFrame(modelling_data, columns=["Section", "Details"])
    st.table(modelling_df)   # âœ… ìë™ ì¤„ë°”ê¿ˆ í‘œ
























    st.header("Modeling/Evaluation")
    
    st.header("Churn Prediction Model Performance Evaluation")

    modeling_path = r"C:\Users\honor\spicedAcademy\Capstone_Final_Project\Retain_Flow_Automation-\notebook\notebook\modeling_insight"

    if os.path.exists(modeling_path):
        img_files = [f for f in os.listdir(modeling_path) if f.endswith((".png", ".jpg", ".jpeg"))]

        if img_files:
            for img in sorted(img_files):  # ì •ë ¬í•´ì„œ ìˆœì„œëŒ€ë¡œ ë³´ì—¬ì£¼ê¸°
                st.image(
                    os.path.join(modeling_path, img),
                    caption=img,
                    width=800 # âœ… ì›í•˜ëŠ” í¬ê¸° (px ë‹¨ìœ„)
                )
        else:
            st.warning("âš ï¸ ëª¨ë¸ ì„±ëŠ¥ ì´ë¯¸ì§€ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
    else:
        st.error("âŒ ëª¨ë¸ë§ ê²°ê³¼ ê²½ë¡œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê²½ë¡œë¥¼ ë‹¤ì‹œ í™•ì¸í•´ì£¼ì„¸ìš”.")












    # ---------------------------
    # Churn ëª¨ë¸ê³¼ Revenue ëª¨ë¸ êµ¬ë¶„ì„ 
    # ---------------------------
    st.header("Revenue Model - Methodology and Technology Stack Used")

    revenue_methodology_data = [
        ["Problem Definition", 
         "Predict customer lifetime revenue (TotalCharges) more accurately by combining baseline trends with advanced modeling"],
        ["Data Preparation", 
         "- Source: customer_Info copy.csv\n- Drop customerID\n- Convert TotalCharges to numeric & fill missing values"],
        ["Preprocessing Strategy", 
         "- Baseline: tenure, MonthlyCharges â†’ Linear Regression\n- Residual: categorical features (Contract, PaymentMethod, InternetService, add-ons like TechSupport, OnlineSecurity) â†’ OneHotEncoding"],
        ["Residual Concept", 
         "Residual = Actual TotalCharges â€“ Baseline prediction\nRandomForestRegressor predicts these residuals"],
        ["Process", 
         "1. Baseline model (Linear Regression)\n2. Compute residuals\n3. Train RandomForestRegressor on residuals\n4. Final prediction = Baseline + Residual model"],
        ["Technology Stack", 
         "- Python (pandas, numpy, scikit-learn, seaborn, matplotlib)\n"
         "- Models: LinearRegression + RandomForestRegressor\n"
         "- Pipeline + ColumnTransformer + OneHotEncoder\n"
         "- Metrics: RÂ², RMSE\n"
         "- Visualization: scatter plots, residual histograms, feature importances"]
    ]
    revenue_methodology_df = pd.DataFrame(revenue_methodology_data, columns=["Section", "Details"])
    st.table(revenue_methodology_df)

    st.header("Revenue Model - Modelling")

    revenue_modelling_data = [
        ["Baseline Model", "Linear Regression with tenure Ã— MonthlyCharges"],
        ["Baseline Performance", "RÂ² â‰ˆ 0.89 â†’ explains ~89% of revenue variance"],
        ["Residual Modeling", "RandomForestRegressor trained on categorical features (Contract, PaymentMethod, TechSupport, etc.)"],
        ["Residual Performance", "RÂ² â‰ˆ 0.55 â†’ explains ~55% of variance in residuals"],
        ["Final Model", "Final prediction = Baseline + Residual model"],
        ["Final Performance", "RÂ² â‰ˆ 0.965, RMSE â‰ˆ 424\nAverage revenue â‰ˆ 2280 â†’ error â‰ˆ 18.6%"],
        ["Feature Importance (Residual Model)", "Key drivers: Contract type, InternetService, PaymentMethod, TechSupport, OnlineSecurity"],
        ["Visualization", "- Baseline vs Actual (scatter)\n- Residual distribution (histogram)\n- Residual Feature Importances (barplot)\n- Final Actual vs Predicted (scatter)\n- Final Residuals (histogram)"]
    ]
    revenue_modelling_df = pd.DataFrame(revenue_modelling_data, columns=["Section", "Details"])
    st.table(revenue_modelling_df)












    st.divider()  # ìµœì‹  Streamlit
    # st.markdown("---")  # í˜¹ì€ ì´ ë°©ì‹ë„ ê°€ëŠ¥

    st.header("Revenue Prediction Model Performance Evaluation")

    revenue_path = r"C:\Users\honor\spicedAcademy\Capstone_Final_Project\Retain_Flow_Automation-\notebook\revenue_insight"

    if os.path.exists(revenue_path):
        img_files = [f for f in os.listdir(revenue_path) if f.endswith(".png")]
        if img_files:
            for img in sorted(img_files):
                st.image(
                    os.path.join(revenue_path, img),
                    caption=img,
                    width=800
                )
        else:
            st.warning("âš ï¸ Revenue ëª¨ë¸ ì‹œê°í™” ì´ë¯¸ì§€ê°€ ì—†ìŠµë‹ˆë‹¤.")
    else:
        st.error("âŒ Revenue ê²°ê³¼ ê²½ë¡œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê²½ë¡œë¥¼ ë‹¤ì‹œ í™•ì¸í•´ì£¼ì„¸ìš”.")







with tab4:
    st.header("Customer Churn + Revenue Forecasting (Supabase Integration)")

    # ---------------------------
    # 1. ëª¨ë¸ ë¡œë“œ
    # ---------------------------
    with open("notebook/pipeline_customer_churn_model.pkl", "rb") as f:
        bundle = pickle.load(f)

    model = bundle["model"]
    scaler = bundle["scaler"]
    kmeans = bundle["kmeans"]

    with open("notebook/pipeline_customer_revenue_model.pkl", "rb") as f:
        revenue_bundle = pickle.load(f)

    base_model = revenue_bundle["baseline_model"]
    residual_model = revenue_bundle["residual_model"]

    # ---------------------------
    # 2. Postgres DB ì—°ê²°
    # ---------------------------
    engine = create_engine( "postgresql://postgres:Nwk5JYywxV3ATT8M@db.fjaxvaegmtbsyogavuzy.supabase.co:5432/postgres" )

    # ---------------------------
    # 4. Streamlit UI
    # ---------------------------
    uploaded_file = st.file_uploader("Customer CSV upload", type="csv")

    if uploaded_file:
        df = pd.read_csv(uploaded_file)

        # (1) ê³ ê° ì´íƒˆ í™•ë¥  ì˜ˆì¸¡
        df["churn_prob"] = model.predict_proba(df)[:, 1]

        # (2) ë§¤ì¶œ ì˜ˆì¸¡ (Baseline + Residual)
        X_base = df[["tenure", "MonthlyCharges"]]
        X_res = df[["PaymentMethod", "PaperlessBilling", "Dependents",
                    "OnlineBackup", "InternetService", "StreamingTV", "OnlineSecurity"]]

        baseline_pred = base_model.predict(X_base)
        residual_pred = residual_model.predict(X_res)
        df["predicted_revenue"] = np.clip(baseline_pred + residual_pred, a_min=0, a_max=None)

        # (3) ğŸ“Œ 12ê°œì›” ê¸°ì¤€ ì§€í‘œ
        df["revenue_12m"] = df["MonthlyCharges"] * 12
        df["expected_loss_12m"] = df["revenue_12m"] * df["churn_prob"]

        # (4) í´ëŸ¬ìŠ¤í„°ë§
        cluster_input = pd.DataFrame({
            "ChurnProbability": df["churn_prob"],
            "MonthlyCharges": df["MonthlyCharges"]
        })
        df["Cluster"] = kmeans.predict(scaler.transform(cluster_input))

        # ğŸ“Š í´ëŸ¬ìŠ¤í„°ë³„ í‰ê· ìœ¼ë¡œ Risk/Value ìë™ ë¼ë²¨ë§
        cluster_summary = df.groupby("Cluster")[["churn_prob", "MonthlyCharges"]].mean()
        risk_threshold = df["churn_prob"].mean()
        value_threshold = df["MonthlyCharges"].mean()

        def auto_label(cluster):
            row = cluster_summary.loc[cluster]
            risk = "High Risk" if row["churn_prob"] >= risk_threshold else "Low Risk"
            value = "High Value" if row["MonthlyCharges"] >= value_threshold else "Low Value"
            return f"{risk} & {value}"

        df["cluster_label"] = df["Cluster"].apply(auto_label)

        # (5) base_message ìƒì„±
        base_messages = {
            "High Risk & High Value": "Exclusive premium offers to retain our top customers",
            "High Risk & Low Value": "Special discount to prevent churn at minimal cost",
            "Low Risk & High Value": "VIP thank-you campaign for loyal high-value customers",
            "Low Risk & Low Value": "Customer feedback request to strengthen relationships",
        }
        df["base_message"] = df["cluster_label"].map(base_messages)


        # (6) ì»¬ëŸ¼ëª… DB í…Œì´ë¸”ê³¼ ë§ì¶”ê¸°
        df = df.rename(columns={
            "customerID": "customer_id",
            "Email": "email"
        })


        # ---------------------------
        # 7. Supabase DB ì €ì¥ (ì „ì²´ ê³ ê° â†’ predictions í…Œì´ë¸”)
        # ---------------------------
        metadata = MetaData()
        metadata.reflect(bind=engine)
        predictions_table = metadata.tables["predictions"]

        with engine.begin() as conn:
            for _, row in df.iterrows():
                stmt = insert(predictions_table).values(
                    customer_id=row["customer_id"],
                    email=row["email"],
                    churn_prob=row["churn_prob"],
                    cluster_label=row["cluster_label"],
                    base_message=row["base_message"],
                    predicted_revenue=row["predicted_revenue"],
                    revenue_12m=row["revenue_12m"],
                    expected_loss_12m=row["expected_loss_12m"]
                )
                stmt = stmt.on_conflict_do_update(
                    index_elements=["customer_id"],
                    set_={
                        "email": row["email"],
                        "churn_prob": row["churn_prob"],
                        "cluster_label": row["cluster_label"],
                        "base_message": row["base_message"],
                        "predicted_revenue": row["predicted_revenue"],
                        "revenue_12m": row["revenue_12m"],
                        "expected_loss_12m": row["expected_loss_12m"]
                    }
                )
                conn.execute(stmt)

        # ---------------------------
        # 8. Top 10 ê³ ê° ì €ì¥ (â†’ top_risk_customers í…Œì´ë¸”)
        # ---------------------------
        top10 = df.sort_values("expected_loss_12m", ascending=False).head(10)

        top_table = metadata.tables["top_risk_customers"]

        with engine.begin() as conn:
            # ê¸°ì¡´ ë°ì´í„° ì§€ìš°ê³  ìƒˆë¡œ ì €ì¥ (ë®ì–´ì“°ê¸° ë°©ì‹)
            conn.execute(text("TRUNCATE TABLE top_risk_customers;"))

            for _, row in top10.iterrows():
                stmt = insert(top_table).values(
                    customer_id=row["customer_id"],
                    email=row["email"],
                    churn_prob=row["churn_prob"],
                    cluster_label=row["cluster_label"],
                    base_message=row["base_message"],
                    predicted_revenue=row["predicted_revenue"],
                    revenue_12m=row["revenue_12m"],
                    expected_loss_12m=row["expected_loss_12m"]
                )
                conn.execute(stmt)

        # ---------------------------
        # 9. Streamlit ì¶œë ¥
        # ---------------------------
        st.subheader("ì˜ˆì¸¡ ë° ì„¸ê·¸ë¨¼íŠ¸ ê²°ê³¼")
        st.dataframe(df[["customer_id", "email", "churn_prob",
                        "cluster_label", "base_message",
                        "predicted_revenue",
                        "revenue_12m", "expected_loss_12m"]])

        st.subheader("Top 10 Revenue at Risk ê³ ê° (12M ê¸°ì¤€)")
        st.dataframe(top10)

        st.success("âœ… Supabase DB ì—…ë°ì´íŠ¸ ì™„ë£Œ! (ì „ì²´ predictions + Top 10 ì €ì¥)")




with tab5:

    st.header("Next Steps & Open Challenges")

    st.markdown("""
- **Data expansion**: Include call center logs, customer support chats, and usage data.  
- **Real-time integration**: Connect models directly with CRM for live churn alerts.  
- **A/B testing**: Validate the effectiveness of personalized retention campaigns.  
- **Churn model optimization**: Explore XGBoost/LightGBM, better calibration, and automated threshold tuning.  
- **Model robustness**: How well does the model generalize across new customer cohorts?  
- **Ethics & fairness**: Could targeting strategies unintentionally bias or exclude groups?  
    """)

    st.divider()

    st.header("Long-term Vision")
    st.markdown("""
- Build **automation pipelines** with high-performing models beyond churn/revenue.  
- Enable **1 person to deliver the productivity of 10** through intelligent automation.  
- Move toward a future where **data-driven decision-making** is seamlessly embedded in daily operations.  
    """)

    st.markdown(
        """
        <sub>Note: This project is a first step toward scaling intelligent automation across business functions.</sub>
        """,
        unsafe_allow_html=True
    )

















