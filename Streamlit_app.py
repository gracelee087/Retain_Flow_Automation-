import pandas as pd
import streamlit as st
import pickle
from sqlalchemy import create_engine, MetaData
from sqlalchemy.dialects.postgresql import insert
import numpy as np
from dateutil.relativedelta import relativedelta

# ---------------------------
# 1. ëª¨ë¸ ë¡œë“œ
# ---------------------------
with open("notebook/pipeline_customer_churn_model.pkl", "rb") as f:
    bundle = pickle.load(f)

model = bundle["model"]
scaler = bundle["scaler"]
kmeans = bundle["kmeans"]

with open("notebook/pipeline_customer_revenue_model.pkl", "rb") as f:
    revenue_bundle = pickle.load(f)

base_model = revenue_bundle["baseline_model"]
residual_model = revenue_bundle["residual_model"]

# ---------------------------
# 2. Postgres DB ì—°ê²°
# ---------------------------
engine = create_engine(
    "postgresql://postgres:PqKHbS8fqXKSnyYv@db.fjaxvaegmtbsyogavuzy.supabase.co:5432/postgres"
)

# ---------------------------
# 3. ì„¸ê·¸ë¨¼íŠ¸ ë¼ë²¨ë§ í•¨ìˆ˜ & base_message ë§¤í•‘
# ---------------------------
def label_cluster(cluster):
    if cluster == 2:
        return "High Risk & High Value"
    elif cluster == 0:
        return "Low Risk & High Value"
    elif cluster == 1:
        return "Low Risk & Low Value"
    elif cluster == 3:
        return "Low Risk & Mid Value"
    else:
        return "Unknown"

base_messages = {
    "High Risk & High Value": "í”„ë¦¬ë¯¸ì—„ ê³ ê° ì „ìš© í˜œíƒ ì•ˆë‚´",
    "Low Risk & High Value": "VIP ê³ ê°ë‹˜ê»˜ ë“œë¦¬ëŠ” ê°ì‚¬ ì¸ì‚¬",
    "Low Risk & Low Value": "ê³ ê°ë‹˜ì˜ ì†Œì¤‘í•œ ì˜ê²¬ì„ ë“£ê³  ì‹¶ìŠµë‹ˆë‹¤",
    "Low Risk & Mid Value": "í¸ì•ˆí•œ ì„œë¹„ìŠ¤ ì´ìš©ì„ ìœ„í•œ ë§ì¶¤ ì œì•ˆ",
    "Unknown": "ê¸°ë³¸ ì•ˆë‚´ ë©”ì‹œì§€"
}

# ---------------------------
# 4. Streamlit UI
# ---------------------------
st.title("ğŸ“Š ê³ ê° ì´íƒˆ + ë§¤ì¶œ ì˜ˆì¸¡ (Supabase ì—°ë™)")

uploaded_file = st.file_uploader("ê³ ê° CSV ì—…ë¡œë“œ", type="csv")

if uploaded_file:
    df = pd.read_csv(uploaded_file)

    # (1) ê³ ê° ì´íƒˆ í™•ë¥  ì˜ˆì¸¡
    df["churn_prob"] = model.predict_proba(df)[:, 1]

    # (2) ë§¤ì¶œ ì˜ˆì¸¡ (Baseline + Residual â†’ LTV ê¸°ë°˜)
    X_base = df[["tenure", "MonthlyCharges"]]
    X_res = df[["PaymentMethod", "PaperlessBilling", "Dependents",
                "OnlineBackup", "InternetService", "StreamingTV", "OnlineSecurity"]]

    baseline_pred = base_model.predict(X_base)
    residual_pred = residual_model.predict(X_res)
    df["predicted_revenue"] = np.clip(baseline_pred + residual_pred, a_min=0, a_max=None)

    # (3) Revenue at Risk (Expected Loss, LTV ê¸°ì¤€)
    df["expected_loss"] = df["churn_prob"] * df["predicted_revenue"]

    # (3-b) ğŸ“Œ 12ê°œì›” ê¸°ì¤€ ì§€í‘œ ì¶”ê°€ (Telco í‘œì¤€)
    df["revenue_12m"] = df["MonthlyCharges"] * 12
    df["expected_loss_12m"] = df["revenue_12m"] * df["churn_prob"]

    # (3-c) LTV ê·¼ì‚¬ì¹˜ (ê°„ë‹¨íˆ 1/churn_prob ê°œì›” ë‚¨ëŠ”ë‹¤ê³  ê°€ì •)
    df["expected_months_remaining"] = df["churn_prob"].apply(lambda p: 1/p if p > 0 else 60)  # ìµœëŒ€ 60ê°œì›” cap
    df["ltv"] = df["MonthlyCharges"] * df["expected_months_remaining"]
    df["expected_loss_ltv"] = df["ltv"] * df["churn_prob"]

    # (3-d) ê°€ì…ì¼(start_date) ì¶”ì •
    today = pd.to_datetime("2025-09-01")  # ê¸°ì¤€ì¼
    df["start_date"] = df["tenure"].apply(lambda m: today - relativedelta(months=int(m)))

    # (4) í´ëŸ¬ìŠ¤í„°ë§
    cluster_input = pd.DataFrame({
        "ChurnProbability": df["churn_prob"],
        "MonthlyCharges": df["MonthlyCharges"]
    })
    df["Cluster"] = kmeans.predict(scaler.transform(cluster_input))
    df["cluster_label"] = df["Cluster"].apply(label_cluster)

    # (5) base_message ìƒì„±
    df["base_message"] = df["cluster_label"].map(base_messages)

    # (6) ì»¬ëŸ¼ëª… DB í…Œì´ë¸”ê³¼ ë§ì¶”ê¸°
    df = df.rename(columns={
        "customerID": "customer_id",
        "Email": "email"
    })

    # (7) Supabase DB ì €ì¥ (UPSERT)
    metadata = MetaData()
    metadata.reflect(bind=engine)
    predictions_table = metadata.tables["predictions"]

    with engine.begin() as conn:
        for _, row in df.iterrows():
            stmt = insert(predictions_table).values(
                customer_id=row["customer_id"],
                email=row["email"],
                churn_prob=row["churn_prob"],
                cluster_label=row["cluster_label"],
                base_message=row["base_message"],
                predicted_revenue=row["predicted_revenue"],     # LTV ê¸°ë°˜
                expected_loss=row["expected_loss"],             # LTV ê¸°ë°˜
                revenue_12m=row["revenue_12m"],                 # 12M ê¸°ì¤€
                expected_loss_12m=row["expected_loss_12m"],     # 12M ê¸°ì¤€
                ltv=row["ltv"],                                 # ìƒì•  ê°€ì¹˜
                expected_loss_ltv=row["expected_loss_ltv"],     # LTV ì†ì‹¤ ìœ„í—˜
                start_date=row["start_date"]
            )
            stmt = stmt.on_conflict_do_update(
                index_elements=["customer_id"],
                set_={
                    "email": row["email"],
                    "churn_prob": row["churn_prob"],
                    "cluster_label": row["cluster_label"],
                    "base_message": row["base_message"],
                    "predicted_revenue": row["predicted_revenue"],
                    "expected_loss": row["expected_loss"],
                    "revenue_12m": row["revenue_12m"],
                    "expected_loss_12m": row["expected_loss_12m"],
                    "ltv": row["ltv"],
                    "expected_loss_ltv": row["expected_loss_ltv"],
                    "start_date": row["start_date"]
                }
            )
            conn.execute(stmt)

    # (8) Streamlit ì¶œë ¥
    st.subheader("ì˜ˆì¸¡ ë° ì„¸ê·¸ë¨¼íŠ¸ ê²°ê³¼")
    st.dataframe(df[["customer_id", "email", "churn_prob",
                     "cluster_label", "base_message",
                     "predicted_revenue", "expected_loss",
                     "revenue_12m", "expected_loss_12m",
                     "ltv", "expected_loss_ltv",
                     "start_date"]])

    st.subheader("Top 10 Revenue at Risk ê³ ê° (LTV ê¸°ì¤€)")
    st.dataframe(df.sort_values("expected_loss", ascending=False).head(10))

    st.subheader("Top 10 Revenue at Risk ê³ ê° (12M ê¸°ì¤€)")
    st.dataframe(df.sort_values("expected_loss_12m", ascending=False).head(10))

    st.success("âœ… Supabase DBì— ìµœì‹  ì •ë³´ ì €ì¥ ì™„ë£Œ! (ì¤‘ë³µ ê³ ê°ì€ ì—…ë°ì´íŠ¸ë¨)")































# 2025 09 02 - (2) 
# import pandas as pd
# import streamlit as st
# import pickle
# from sqlalchemy import create_engine, MetaData
# from sqlalchemy.dialects.postgresql import insert
# import numpy as np   # âœ… ì¶”ê°€



# # ---------------------------
# # 1. ëª¨ë¸ ë¡œë“œ
# # ---------------------------
# # (1) ê³ ê° ì´íƒˆ ì˜ˆì¸¡ ëª¨ë¸
# with open("notebook/pipeline_customer_churn_model.pkl", "rb") as f:
#     bundle = pickle.load(f)

# model = bundle["model"]
# scaler = bundle["scaler"]
# kmeans = bundle["kmeans"]

# # (2) ê³ ê° ë§¤ì¶œ ì˜ˆì¸¡ ëª¨ë¸
# with open("notebook/pipeline_customer_revenue_model.pkl", "rb") as f:
#     revenue_bundle = pickle.load(f)

# base_model = revenue_bundle["baseline_model"]
# residual_model = revenue_bundle["residual_model"]

# # ---------------------------
# # 2. Postgres DB ì—°ê²°
# # ---------------------------
# engine = create_engine(
#     "postgresql://postgres:PqKHbS8fqXKSnyYv@db.fjaxvaegmtbsyogavuzy.supabase.co:5432/postgres"
# )

# # ---------------------------
# # 3. ì„¸ê·¸ë¨¼íŠ¸ ë¼ë²¨ë§ í•¨ìˆ˜ & base_message ë§¤í•‘
# # ---------------------------
# def label_cluster(cluster):
#     if cluster == 2:
#         return "High Risk & High Value"
#     elif cluster == 0:
#         return "Low Risk & High Value"
#     elif cluster == 1:
#         return "Low Risk & Low Value"
#     elif cluster == 3:
#         return "Low Risk & Mid Value"
#     else:
#         return "Unknown"

# base_messages = {
#     "High Risk & High Value": "í”„ë¦¬ë¯¸ì—„ ê³ ê° ì „ìš© í˜œíƒ ì•ˆë‚´",
#     "Low Risk & High Value": "VIP ê³ ê°ë‹˜ê»˜ ë“œë¦¬ëŠ” ê°ì‚¬ ì¸ì‚¬",
#     "Low Risk & Low Value": "ê³ ê°ë‹˜ì˜ ì†Œì¤‘í•œ ì˜ê²¬ì„ ë“£ê³  ì‹¶ìŠµë‹ˆë‹¤",
#     "Low Risk & Mid Value": "í¸ì•ˆí•œ ì„œë¹„ìŠ¤ ì´ìš©ì„ ìœ„í•œ ë§ì¶¤ ì œì•ˆ",
#     "Unknown": "ê¸°ë³¸ ì•ˆë‚´ ë©”ì‹œì§€"
# }

# # ---------------------------
# # 4. Streamlit UI
# # ---------------------------
# st.title("ğŸ“Š ê³ ê° ì´íƒˆ + ë§¤ì¶œ ì˜ˆì¸¡ (Supabase ì—°ë™)")

# uploaded_file = st.file_uploader("ê³ ê° CSV ì—…ë¡œë“œ", type="csv")

# if uploaded_file:
#     df = pd.read_csv(uploaded_file)

#     # (1) ê³ ê° ì´íƒˆ í™•ë¥  ì˜ˆì¸¡
#     df["churn_prob"] = model.predict_proba(df)[:, 1]

#     # (2) ë§¤ì¶œ ì˜ˆì¸¡ (Baseline + Residual)
#     X_base = df[["tenure", "MonthlyCharges"]]
#     X_res = df[["PaymentMethod", "PaperlessBilling", "Dependents",
#                 "OnlineBackup", "InternetService", "StreamingTV", "OnlineSecurity"]]

#     baseline_pred = base_model.predict(X_base)
#     residual_pred = residual_model.predict(X_res)



# # âœ… ë§¤ì¶œì€ ìŒìˆ˜ê°€ ë  ìˆ˜ ì—†ìœ¼ë¯€ë¡œ NumPy clip ì‚¬ìš©
#     df["predicted_revenue"] = np.clip(baseline_pred + residual_pred, a_min=0, a_max=None)

# # (3) Revenue at Risk (Expected Loss)
#     df["expected_loss"] = df["churn_prob"] * df["predicted_revenue"]



# # âœ… ì›” ë‹¨ìœ„ ì§€í‘œ ì¶”ê°€
#     df["monthly_predicted_revenue"] = df.apply(
#     lambda row: row["predicted_revenue"] / row["tenure"] if row["tenure"] > 0 else 0,
#     axis=1
#     )

#     df["monthly_expected_loss"] = df.apply(
#     lambda row: row["expected_loss"] / row["tenure"] if row["tenure"] > 0 else 0,
#     axis=1
#     )




#     # (4) í´ëŸ¬ìŠ¤í„°ë§
#     cluster_input = pd.DataFrame({
#         "ChurnProbability": df["churn_prob"],
#         "MonthlyCharges": df["MonthlyCharges"]
#     })
#     df["Cluster"] = kmeans.predict(scaler.transform(cluster_input))
#     df["cluster_label"] = df["Cluster"].apply(label_cluster)

#     # (5) base_message ìƒì„±
#     df["base_message"] = df["cluster_label"].map(base_messages)

#     # (6) ì»¬ëŸ¼ëª… DB í…Œì´ë¸”ê³¼ ë§ì¶”ê¸°
#     df = df.rename(columns={
#         "customerID": "customer_id",
#         "Email": "email"
#     })

#     # (7) Supabase DB ì €ì¥ (UPSERT)
#     metadata = MetaData()
#     metadata.reflect(bind=engine)
#     predictions_table = metadata.tables["predictions"]

#     with engine.begin() as conn:
#         for _, row in df.iterrows():
#             stmt = insert(predictions_table).values(
#                 customer_id=row["customer_id"],
#                 email=row["email"],
#                 churn_prob=row["churn_prob"],
#                 cluster_label=row["cluster_label"],
#                 base_message=row["base_message"],
#                 predicted_revenue=row["predicted_revenue"],
#                 expected_loss=row["expected_loss"]
#             )
#             # âœ… customer_idê°€ ì´ë¯¸ ìˆìœ¼ë©´ ì—…ë°ì´íŠ¸
#             stmt = stmt.on_conflict_do_update(
#                 index_elements=["customer_id"],
#                 set_={
#                     "email": row["email"],
#                     "churn_prob": row["churn_prob"],
#                     "cluster_label": row["cluster_label"],
#                     "base_message": row["base_message"],
#                     "predicted_revenue": row["predicted_revenue"],
#                     "expected_loss": row["expected_loss"]
#                 }
#             )
#             conn.execute(stmt)

#     # (8) Streamlit ì¶œë ¥
#     st.subheader("ì˜ˆì¸¡ ë° ì„¸ê·¸ë¨¼íŠ¸ ê²°ê³¼")
#     st.dataframe(df[["customer_id", "email", "churn_prob",
#                      "cluster_label", "base_message",
#                      "predicted_revenue", "expected_loss"]])

#     st.subheader("Top 10 Revenue at Risk ê³ ê°")
#     st.dataframe(df.sort_values("expected_loss", ascending=False).head(10))

#     st.success("âœ… Supabase DBì— ìµœì‹  ì •ë³´ ì €ì¥ ì™„ë£Œ! (ì¤‘ë³µ ê³ ê°ì€ ì—…ë°ì´íŠ¸ë¨)")

























# 2025 / 09 / 02 - (1)
# import pandas as pd
# import streamlit as st
# import pickle
# from sqlalchemy import create_engine, MetaData
# from sqlalchemy.dialects.postgresql import insert

# # ---------------------------
# # 1. ëª¨ë¸ ë¡œë“œ
# # ---------------------------
# with open("notebook/pipeline_customer_churn_model.pkl", "rb") as f:
#     bundle = pickle.load(f)

# model = bundle["model"]
# scaler = bundle["scaler"]
# kmeans = bundle["kmeans"]

# # ---------------------------
# # 2. Postgres DB ì—°ê²°
# # ---------------------------
# engine = create_engine(
#     "postgresql://postgres:PqKHbS8fqXKSnyYv@db.fjaxvaegmtbsyogavuzy.supabase.co:5432/postgres"
# )

# # ---------------------------
# # 3. ì„¸ê·¸ë¨¼íŠ¸ ë¼ë²¨ë§ í•¨ìˆ˜ & base_message ë§¤í•‘
# # ---------------------------
# def label_cluster(cluster):
#     if cluster == 2:
#         return "High Risk & High Value"
#     elif cluster == 0:
#         return "Low Risk & High Value"
#     elif cluster == 1:
#         return "Low Risk & Low Value"
#     elif cluster == 3:
#         return "Low Risk & Mid Value"
#     else:
#         return "Unknown"

# base_messages = {
#     "High Risk & High Value": "í”„ë¦¬ë¯¸ì—„ ê³ ê° ì „ìš© í˜œíƒ ì•ˆë‚´",
#     "Low Risk & High Value": "VIP ê³ ê°ë‹˜ê»˜ ë“œë¦¬ëŠ” ê°ì‚¬ ì¸ì‚¬",
#     "Low Risk & Low Value": "ê³ ê°ë‹˜ì˜ ì†Œì¤‘í•œ ì˜ê²¬ì„ ë“£ê³  ì‹¶ìŠµë‹ˆë‹¤",
#     "Low Risk & Mid Value": "í¸ì•ˆí•œ ì„œë¹„ìŠ¤ ì´ìš©ì„ ìœ„í•œ ë§ì¶¤ ì œì•ˆ",
#     "Unknown": "ê¸°ë³¸ ì•ˆë‚´ ë©”ì‹œì§€"
# }

# # ---------------------------
# # 4. Streamlit UI
# # ---------------------------
# st.title("ê³ ê° ì´íƒˆ ì˜ˆì¸¡ + ì„¸ê·¸ë¨¼íŠ¸ ë°ëª¨ (Supabase ë²„ì „)")

# uploaded_file = st.file_uploader("ê³ ê° CSV ì—…ë¡œë“œ", type="csv")

# if uploaded_file:
#     df = pd.read_csv(uploaded_file)

#     # (1) ê³ ê° ì´íƒˆ í™•ë¥  ì˜ˆì¸¡
#     df["churn_prob"] = model.predict_proba(df)[:, 1]

#     # (2) í´ëŸ¬ìŠ¤í„°ë§
#     cluster_input = pd.DataFrame({
#         "ChurnProbability": df["churn_prob"],
#         "MonthlyCharges": df["MonthlyCharges"]
#     })
#     df["Cluster"] = kmeans.predict(scaler.transform(cluster_input))
#     df["cluster_label"] = df["Cluster"].apply(label_cluster)

#     # (3) base_message ìƒì„±
#     df["base_message"] = df["cluster_label"].map(base_messages)

#     # (4) ì»¬ëŸ¼ëª… DB í…Œì´ë¸”ê³¼ ë§ì¶”ê¸°
#     df = df.rename(columns={
#         "customerID": "customer_id",
#         "Email": "email"
#     })

#     # (5) Supabase DB ì €ì¥ (UPSERT)
#     metadata = MetaData()
#     metadata.reflect(bind=engine)
#     predictions_table = metadata.tables["predictions"]

#     with engine.begin() as conn:
#         for _, row in df.iterrows():
#             stmt = insert(predictions_table).values(
#                 customer_id=row["customer_id"],
#                 email=row["email"],
#                 churn_prob=row["churn_prob"],
#                 cluster_label=row["cluster_label"],
#                 base_message=row["base_message"]
#             )
#             # âœ… customer_idê°€ ì´ë¯¸ ìˆìœ¼ë©´ ì—…ë°ì´íŠ¸
#             stmt = stmt.on_conflict_do_update(
#                 index_elements=["customer_id"],
#                 set_={
#                     "email": row["email"],
#                     "churn_prob": row["churn_prob"],
#                     "cluster_label": row["cluster_label"],
#                     "base_message": row["base_message"]
#                 }
#             )
#             conn.execute(stmt)

#     # (6) Streamlit ì¶œë ¥
#     st.subheader("ì˜ˆì¸¡ ë° ì„¸ê·¸ë¨¼íŠ¸ ê²°ê³¼")
#     st.dataframe(df[["customer_id", "email", "churn_prob", "cluster_label", "base_message"]])

#     st.success("âœ… Supabase DBì— ìµœì‹  ì •ë³´ ì €ì¥ ì™„ë£Œ! (ì¤‘ë³µ ê³ ê°ì€ ì—…ë°ì´íŠ¸ë¨)")



































# âœ… customers.db í•‘í¬ íŠ¹ì§•

# íŒŒì¼ ê¸°ë°˜: .db íŒŒì¼ í•˜ë‚˜ = ë°ì´í„°ë² ì´ìŠ¤ ì „ì²´ (í…Œì´ë¸”, ì¸ë±ìŠ¤, ë°ì´í„° ë‹¤ í¬í•¨)

# ì„œë²„ ë¶ˆí•„ìš”: Postgres/MySQLì²˜ëŸ¼ ì„œë²„ë¥¼ ì¼œì§€ ì•Šì•„ë„ ë¨ â†’ Pythonë§Œ ìˆìœ¼ë©´ ë¨

# SQL ê·¸ëŒ€ë¡œ ì‚¬ìš© ê°€ëŠ¥: SELECT, INSERT, UPDATE ê°™ì€ SQL ë¬¸ë²• 100% ì§€ì›

# ì´ì‹ì„± ìµœê³ : íŒŒì¼ í•˜ë‚˜ë§Œ ë³µì‚¬í•˜ë©´ ë‹¤ë¥¸ PCì—ì„œë„ ë°”ë¡œ ì‚¬ìš© ê°€ëŠ¥

# âœ¨ ì‹ ê¸°í•œ ì 

# Streamlit â†’ Pandas DataFrame â†’ SQLite .db ì €ì¥ â†’ VS Codeì—ì„œ ë°”ë¡œ í…Œì´ë¸” ì¡°íšŒ

# ì¦‰, **ì—‘ì…€ íŒŒì¼ì²˜ëŸ¼ ê°€ë³ê²Œ ì“°ëŠ”ë°, SQL DBì˜ ì¥ì (ì¿¼ë¦¬, ìŠ¤í‚¤ë§ˆ, í™•ì¥ì„±)**ì„ ë™ì‹œì— ëˆ„ë¦´ ìˆ˜ ìˆìŒ